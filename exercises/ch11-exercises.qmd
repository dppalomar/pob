---
title: "Chapter 11 - Risk Parity Portfolio"
subtitle: "Exercises"
format:
  pdf:
    pdf-engine: pdflatex
    include-in-header:
      text: |
        \usepackage{bm}
        \newcounter{chapter}
        \renewcommand\thechapter{11}
        \newtheorem{exercise}{Exercise}[chapter]
        \def\T{{\mkern-2mu\mathsf{T}}}
        \def\bSigma{\bm{\Sigma}}
        \def\textm{\mathsf}
editor: source
---


\begin{exercise}
Show why $\bSigma\bm{x} = \bm{b}/\bm{x}$ can be equivalently solved as $\bm{C}\bm{x} = \bm{b}/\bm{x}$, where $\bm{C}$ is the correlation matrix defined as $\bm{C} = \bm{D}^{-1/2}\bSigma\bm{D}^{-1/2}$ with $\bm{D}$ a diagonal matrix containing $\textm{diag}(\bSigma)$ along the main diagonal. Would it be possible to use instead $\bm{C} = \bm{M}^{-1/2}\bSigma\bm{M}^{-1/2}$, where $\bm{M}$ is not necessaryly a diagonal matrix?
\end{exercise}


\begin{exercise}
If the covariance matrix is diagonal $\bSigma = \bm{D}$, then the system of nonlinear equations $\bSigma\bm{x} = \bm{b}/\bm{x}$ has the closed-form solution $\bm{x} = \sqrt{\bm{b}/\textm{diag}(\bm{D})}$. Explore whether a closed-form solution can be obtained for the rank-one plus diagonal case $\bSigma = \bm{u}\bm{u}^\T + \bm{D}$.
\end{exercise}


\begin{exercise}
The solution to the formulation
$$
  \begin{array}{ll}
  \underset{\bm{x}\ge\bm{0}}{\textm{maximize}} & \bm{b}^\T\log(\bm{x})\\
  \textm{subject to} & \sqrt{\bm{x}^\T\bSigma\bm{x}} \le \sigma_0
  \end{array}
$$
is $$\lambda\bSigma\bm{x} = \bm{b}/\bm{x} \times \sqrt{\bm{x}^\T\bSigma\bm{x}}.$$ Can you solve for $\lambda$ and rewrite the solution in a more compact way without $\lambda$?
\end{exercise}


\begin{exercise}
Newton's method requires computing the direction $\bm{d} = \mathsf{H}^{-1}\nabla f$ or, equivalently, solving the system of linear equations $\mathsf{H}\bm{d} = \nabla f$ for $\bm{d}$. Explore whether a more efficient solution is possible exploiting the structure of the gradient and Hessian:
$$
\begin{aligned}
\nabla f   &= \bSigma\bm{x} - \bm{b}/\bm{x}\\
\mathsf{H} &= \bSigma + \textm{Diag}(\bm{b}/\bm{x}^2).
\end{aligned}
$$
\end{exercise}


\begin{exercise}
The MM algorithm requires the computation of the largest eigenvalue $\lambda_\textm{max}$ of matrix $\bSigma$, which can be obtained from the eigenvalue decomposition of the matrix. A more efficient alternative is the \textit{power iteration method}. Program both methods and compare the computational complexity.
\end{exercise}


\begin{exercise}
Consider the vanilla convex formulation
$$
  \begin{array}{ll}
  \underset{\bm{x}\ge\bm{0}}{\textm{minimize}} & \frac{1}{2}\bm{x}^\T\bSigma\bm{x} - \bm{b}^\T\log(\bm{x}).
  \end{array}
$$
Implement the cyclical coordinate descent method and the parallel SCA method in a high-level programming language (e.g., R, Python, Julia, or Matlab) and compare the converge vs CPU time for these two methods. Then, re-implement these two methods in a low-level programming language (e.g., C, C++, C\#, or Rust) and compare the convergence again. Comment on the difference observed.
\end{exercise}
